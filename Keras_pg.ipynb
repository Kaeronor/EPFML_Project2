{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Data loading and preprocessing\n",
    "### Mostly taken from baseline script given in project description\n",
    "1. Images -> square patches of fixed size \n",
    "2. (?) Extract some features from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Problem-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## 1. Functions for image pre/post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "code_folding": [
     0,
     14,
     40,
     49,
     71,
     84,
     89,
     105
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.array(data)\n",
    "\n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.array([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.array([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx][0] > 0.5:\n",
    "                l = 1\n",
    "            else:\n",
    "                l = 0\n",
    "            array_labels[j:j+w, i:i+h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*PIXEL_DEPTH\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## . Some summary functions for tensorboard, not needed right now (while using keras?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_image_summary(img, idx = 0):\\n    V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\\n    img_w = img.get_shape().as_list()[1]\\n    img_h = img.get_shape().as_list()[2]\\n    min_value = tf.reduce_min(V)\\n    V = V - min_value\\n    max_value = tf.reduce_max(V)\\n    V = V / (max_value*PIXEL_DEPTH)\\n    V = tf.reshape(V, (img_w, img_h, 1))\\n    V = tf.transpose(V, (2, 0, 1))\\n    V = tf.reshape(V, (-1, img_w, img_h, 1))\\n    return V\\n\\n# Make an image summary for 3d tensor image with index idx\\ndef get_image_summary_3d(img):\\n    V = tf.slice(img, (0, 0, 0), (1, -1, -1))\\n    img_w = img.get_shape().as_list()[1]\\n    img_h = img.get_shape().as_list()[2]\\n    V = tf.reshape(V, (img_w, img_h, 1))\\n    V = tf.transpose(V, (2, 0, 1))\\n    V = tf.reshape(V, (-1, img_w, img_h, 1))\\n    return V'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make an image summary for 4d tensor image with index idx\n",
    "'''def get_image_summary(img, idx = 0):\n",
    "    V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\n",
    "    img_w = img.get_shape().as_list()[1]\n",
    "    img_h = img.get_shape().as_list()[2]\n",
    "    min_value = tf.reduce_min(V)\n",
    "    V = V - min_value\n",
    "    max_value = tf.reduce_max(V)\n",
    "    V = V / (max_value*PIXEL_DEPTH)\n",
    "    V = tf.reshape(V, (img_w, img_h, 1))\n",
    "    V = tf.transpose(V, (2, 0, 1))\n",
    "    V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "    return V\n",
    "\n",
    "# Make an image summary for 3d tensor image with index idx\n",
    "def get_image_summary_3d(img):\n",
    "    V = tf.slice(img, (0, 0, 0), (1, -1, -1))\n",
    "    img_w = img.get_shape().as_list()[1]\n",
    "    img_h = img.get_shape().as_list()[2]\n",
    "    V = tf.reshape(V, (img_w, img_h, 1))\n",
    "    V = tf.transpose(V, (2, 0, 1))\n",
    "    V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "    return V'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions to make predictions on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get prediction for given input image \n",
    "def get_prediction(img):\n",
    "    data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    data_node = tf.constant(data)\n",
    "    output = tf.nn.softmax(model(data_node))\n",
    "    output_prediction = s.run(output)\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_prediction_with_groundtruth(filename, image_idx):\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    img = mpimg.imread(image_filename)\n",
    "\n",
    "    img_prediction = get_prediction(img)\n",
    "    cimg = concatenate_images(img, img_prediction)\n",
    "\n",
    "    return cimg\n",
    "\n",
    "# Get prediction overlaid on the original image for given input file\n",
    "def get_prediction_with_overlay(filename, image_idx):\n",
    "\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    img = mpimg.imread(image_filename)\n",
    "\n",
    "    img_prediction = get_prediction(img)\n",
    "    oimg = make_img_overlay(img, img_prediction)\n",
    "\n",
    "    return oimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load images from dataset, convert to inputs for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATCH_SIZE = 16\n",
    "TRAINING_SIZE = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "a = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/images/satImage_021.png\n",
      "Loading training/images/satImage_022.png\n",
      "Loading training/images/satImage_023.png\n",
      "Loading training/images/satImage_024.png\n",
      "Loading training/images/satImage_025.png\n",
      "Loading training/images/satImage_026.png\n",
      "Loading training/images/satImage_027.png\n",
      "Loading training/images/satImage_028.png\n",
      "Loading training/images/satImage_029.png\n",
      "Loading training/images/satImage_030.png\n",
      "Loading training/images/satImage_031.png\n",
      "Loading training/images/satImage_032.png\n",
      "Loading training/images/satImage_033.png\n",
      "Loading training/images/satImage_034.png\n",
      "Loading training/images/satImage_035.png\n",
      "Loading training/images/satImage_036.png\n",
      "Loading training/images/satImage_037.png\n",
      "Loading training/images/satImage_038.png\n",
      "Loading training/images/satImage_039.png\n",
      "Loading training/images/satImage_040.png\n",
      "Loading training/images/satImage_041.png\n",
      "Loading training/images/satImage_042.png\n",
      "Loading training/images/satImage_043.png\n",
      "Loading training/images/satImage_044.png\n",
      "Loading training/images/satImage_045.png\n",
      "Loading training/images/satImage_046.png\n",
      "Loading training/images/satImage_047.png\n",
      "Loading training/images/satImage_048.png\n",
      "Loading training/images/satImage_049.png\n",
      "Loading training/images/satImage_050.png\n",
      "Loading training/images/satImage_051.png\n",
      "Loading training/images/satImage_052.png\n",
      "Loading training/images/satImage_053.png\n",
      "Loading training/images/satImage_054.png\n",
      "Loading training/images/satImage_055.png\n",
      "Loading training/images/satImage_056.png\n",
      "Loading training/images/satImage_057.png\n",
      "Loading training/images/satImage_058.png\n",
      "Loading training/images/satImage_059.png\n",
      "Loading training/images/satImage_060.png\n",
      "Loading training/images/satImage_061.png\n",
      "Loading training/images/satImage_062.png\n",
      "Loading training/images/satImage_063.png\n",
      "Loading training/images/satImage_064.png\n",
      "Loading training/images/satImage_065.png\n",
      "Loading training/images/satImage_066.png\n",
      "Loading training/images/satImage_067.png\n",
      "Loading training/images/satImage_068.png\n",
      "Loading training/images/satImage_069.png\n",
      "Loading training/images/satImage_070.png\n",
      "Loading training/images/satImage_071.png\n",
      "Loading training/images/satImage_072.png\n",
      "Loading training/images/satImage_073.png\n",
      "Loading training/images/satImage_074.png\n",
      "Loading training/images/satImage_075.png\n",
      "Loading training/images/satImage_076.png\n",
      "Loading training/images/satImage_077.png\n",
      "Loading training/images/satImage_078.png\n",
      "Loading training/images/satImage_079.png\n",
      "Loading training/images/satImage_080.png\n",
      "Loading training/images/satImage_081.png\n",
      "Loading training/images/satImage_082.png\n",
      "Loading training/images/satImage_083.png\n",
      "Loading training/images/satImage_084.png\n",
      "Loading training/images/satImage_085.png\n",
      "Loading training/images/satImage_086.png\n",
      "Loading training/images/satImage_087.png\n",
      "Loading training/images/satImage_088.png\n",
      "Loading training/images/satImage_089.png\n",
      "Loading training/images/satImage_090.png\n",
      "Loading training/images/satImage_091.png\n",
      "Loading training/images/satImage_092.png\n",
      "Loading training/images/satImage_093.png\n",
      "Loading training/images/satImage_094.png\n",
      "Loading training/images/satImage_095.png\n",
      "Loading training/images/satImage_096.png\n",
      "Loading training/images/satImage_097.png\n",
      "Loading training/images/satImage_098.png\n",
      "Loading training/images/satImage_099.png\n",
      "Loading training/images/satImage_100.png\n",
      "Loading training/groundtruth/satImage_001.png\n",
      "Loading training/groundtruth/satImage_002.png\n",
      "Loading training/groundtruth/satImage_003.png\n",
      "Loading training/groundtruth/satImage_004.png\n",
      "Loading training/groundtruth/satImage_005.png\n",
      "Loading training/groundtruth/satImage_006.png\n",
      "Loading training/groundtruth/satImage_007.png\n",
      "Loading training/groundtruth/satImage_008.png\n",
      "Loading training/groundtruth/satImage_009.png\n",
      "Loading training/groundtruth/satImage_010.png\n",
      "Loading training/groundtruth/satImage_011.png\n",
      "Loading training/groundtruth/satImage_012.png\n",
      "Loading training/groundtruth/satImage_013.png\n",
      "Loading training/groundtruth/satImage_014.png\n",
      "Loading training/groundtruth/satImage_015.png\n",
      "Loading training/groundtruth/satImage_016.png\n",
      "Loading training/groundtruth/satImage_017.png\n",
      "Loading training/groundtruth/satImage_018.png\n",
      "Loading training/groundtruth/satImage_019.png\n",
      "Loading training/groundtruth/satImage_020.png\n",
      "Loading training/groundtruth/satImage_021.png\n",
      "Loading training/groundtruth/satImage_022.png\n",
      "Loading training/groundtruth/satImage_023.png\n",
      "Loading training/groundtruth/satImage_024.png\n",
      "Loading training/groundtruth/satImage_025.png\n",
      "Loading training/groundtruth/satImage_026.png\n",
      "Loading training/groundtruth/satImage_027.png\n",
      "Loading training/groundtruth/satImage_028.png\n",
      "Loading training/groundtruth/satImage_029.png\n",
      "Loading training/groundtruth/satImage_030.png\n",
      "Loading training/groundtruth/satImage_031.png\n",
      "Loading training/groundtruth/satImage_032.png\n",
      "Loading training/groundtruth/satImage_033.png\n",
      "Loading training/groundtruth/satImage_034.png\n",
      "Loading training/groundtruth/satImage_035.png\n",
      "Loading training/groundtruth/satImage_036.png\n",
      "Loading training/groundtruth/satImage_037.png\n",
      "Loading training/groundtruth/satImage_038.png\n",
      "Loading training/groundtruth/satImage_039.png\n",
      "Loading training/groundtruth/satImage_040.png\n",
      "Loading training/groundtruth/satImage_041.png\n",
      "Loading training/groundtruth/satImage_042.png\n",
      "Loading training/groundtruth/satImage_043.png\n",
      "Loading training/groundtruth/satImage_044.png\n",
      "Loading training/groundtruth/satImage_045.png\n",
      "Loading training/groundtruth/satImage_046.png\n",
      "Loading training/groundtruth/satImage_047.png\n",
      "Loading training/groundtruth/satImage_048.png\n",
      "Loading training/groundtruth/satImage_049.png\n",
      "Loading training/groundtruth/satImage_050.png\n",
      "Loading training/groundtruth/satImage_051.png\n",
      "Loading training/groundtruth/satImage_052.png\n",
      "Loading training/groundtruth/satImage_053.png\n",
      "Loading training/groundtruth/satImage_054.png\n",
      "Loading training/groundtruth/satImage_055.png\n",
      "Loading training/groundtruth/satImage_056.png\n",
      "Loading training/groundtruth/satImage_057.png\n",
      "Loading training/groundtruth/satImage_058.png\n",
      "Loading training/groundtruth/satImage_059.png\n",
      "Loading training/groundtruth/satImage_060.png\n",
      "Loading training/groundtruth/satImage_061.png\n",
      "Loading training/groundtruth/satImage_062.png\n",
      "Loading training/groundtruth/satImage_063.png\n",
      "Loading training/groundtruth/satImage_064.png\n",
      "Loading training/groundtruth/satImage_065.png\n",
      "Loading training/groundtruth/satImage_066.png\n",
      "Loading training/groundtruth/satImage_067.png\n",
      "Loading training/groundtruth/satImage_068.png\n",
      "Loading training/groundtruth/satImage_069.png\n",
      "Loading training/groundtruth/satImage_070.png\n",
      "Loading training/groundtruth/satImage_071.png\n",
      "Loading training/groundtruth/satImage_072.png\n",
      "Loading training/groundtruth/satImage_073.png\n",
      "Loading training/groundtruth/satImage_074.png\n",
      "Loading training/groundtruth/satImage_075.png\n",
      "Loading training/groundtruth/satImage_076.png\n",
      "Loading training/groundtruth/satImage_077.png\n",
      "Loading training/groundtruth/satImage_078.png\n",
      "Loading training/groundtruth/satImage_079.png\n",
      "Loading training/groundtruth/satImage_080.png\n",
      "Loading training/groundtruth/satImage_081.png\n",
      "Loading training/groundtruth/satImage_082.png\n",
      "Loading training/groundtruth/satImage_083.png\n",
      "Loading training/groundtruth/satImage_084.png\n",
      "Loading training/groundtruth/satImage_085.png\n",
      "Loading training/groundtruth/satImage_086.png\n",
      "Loading training/groundtruth/satImage_087.png\n",
      "Loading training/groundtruth/satImage_088.png\n",
      "Loading training/groundtruth/satImage_089.png\n",
      "Loading training/groundtruth/satImage_090.png\n",
      "Loading training/groundtruth/satImage_091.png\n",
      "Loading training/groundtruth/satImage_092.png\n",
      "Loading training/groundtruth/satImage_093.png\n",
      "Loading training/groundtruth/satImage_094.png\n",
      "Loading training/groundtruth/satImage_095.png\n",
      "Loading training/groundtruth/satImage_096.png\n",
      "Loading training/groundtruth/satImage_097.png\n",
      "Loading training/groundtruth/satImage_098.png\n",
      "Loading training/groundtruth/satImage_099.png\n",
      "Loading training/groundtruth/satImage_100.png\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, TRAINING_SIZE)\n",
    "train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62500, 16, 16, 3)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 46309.0 c1 = 16191.0\n"
     ]
    }
   ],
   "source": [
    "c0 = np.sum(train_labels[:,0])\n",
    "c1 = np.sum(train_labels[:,1])\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model which is being used now\n",
    "Right now it is taken from Keras tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS)))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_LABELS, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16, 16, 3) (None, 14, 14, 32)\n",
      "(None, 14, 14, 32) (None, 14, 14, 32)\n",
      "(None, 14, 14, 32) (None, 12, 12, 32)\n",
      "(None, 12, 12, 32) (None, 12, 12, 32)\n",
      "(None, 12, 12, 32) (None, 6, 6, 32)\n",
      "(None, 6, 6, 32) (None, 6, 6, 32)\n",
      "(None, 6, 6, 32) (None, 4, 4, 64)\n",
      "(None, 4, 4, 64) (None, 4, 4, 64)\n",
      "(None, 4, 4, 64) (None, 2, 2, 64)\n",
      "(None, 2, 2, 64) (None, 2, 2, 64)\n",
      "(None, 2, 2, 64) (None, 1, 1, 64)\n",
      "(None, 1, 1, 64) (None, 1, 1, 64)\n",
      "(None, 1, 1, 64) (None, 64)\n",
      "(None, 64) (None, 512)\n",
      "(None, 512) (None, 512)\n",
      "(None, 512) (None, 512)\n",
      "(None, 512) (None, 2)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.input_shape,layer.output_shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "62500/62500 [==============================] - 32s - loss: 0.5743 - acc: 0.7405    \n",
      "Epoch 2/5\n",
      "62500/62500 [==============================] - 31s - loss: 0.5722 - acc: 0.7409    \n",
      "Epoch 3/5\n",
      "62500/62500 [==============================] - 35s - loss: 0.5723 - acc: 0.7409    \n",
      "Epoch 4/5\n",
      "62500/62500 [==============================] - 32s - loss: 0.5734 - acc: 0.7409    \n",
      "Epoch 5/5\n",
      "62500/62500 [==============================] - 32s - loss: 0.5724 - acc: 0.7409    \n",
      "62464/62500 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "\n",
    "model.fit(train_data, train_labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "score = model.evaluate(train_data, train_labels, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.57220119856643681, 0.74094399999237059]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_test_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    names = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"test_{}/test_{}\".format(i,i)\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "            names.append(image_filename)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "    return np.array(imgs), names\n",
    "\n",
    "# Get prediction for given input image \n",
    "def keras_prediction(model, img):\n",
    "    data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    output_prediction = model.predict(data)\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def mask_to_submission_strings(im, im_name):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", im_name).group(0))\n",
    "    for j in range(0, im.shape[1], IMG_PATCH_SIZE):\n",
    "        for i in range(0, im.shape[0], IMG_PATCH_SIZE):\n",
    "            patch = im[i:i + IMG_PATCH_SIZE, j:j + IMG_PATCH_SIZE]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, images, images_names):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for ind, fn in enumerate(images):\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn, images_names[ind]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 50\n",
    "test_data_dir = 'test_set_images/'\n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "test_data, file_names = extract_test_data(test_data_dir, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 608, 608, 3)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filename = 'silly_submission.csv'\n",
    "images = [keras_prediction(model, test_data[i]) for i in range(test_data.shape[0])]\n",
    "masks_to_submission(submission_filename, images, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
