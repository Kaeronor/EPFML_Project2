{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LeakyReLU, Input, Reshape, Permute, Average\n",
    "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Lambda, Activation, GlobalAveragePooling2D, Conv2DTranspose, GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Data loading and preprocessing\n",
    "### Mostly taken from baseline script given in project description\n",
    "1. Images -> square patches of fixed size \n",
    "2. (?) Extract some features from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Problem-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "IMG_SIZE = 400\n",
    "PATCH_SIZE = 16\n",
    "WINDOW_SIZE = 32 # 18px - PATCH_SIZE - 18px\n",
    "NB_WINDOWS = (IMG_SIZE/PATCH_SIZE)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## 1. Functions for image pre/post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    tmp_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = Image.open(image_filename)\n",
    "            tmp_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(tmp_imgs)\n",
    "    for i in range(num_images):\n",
    "        imgs.append(np.asarray(tmp_imgs[i]) / 255)\n",
    "        '''imgs.append(np.asarray(tmp_imgs[i].rotate(90)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].rotate(180)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].rotate(-90)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_LEFT_RIGHT)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_TOP_BOTTOM)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].transpose(Image.TRANSPOSE)) / 255)'''\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "    print('Patches per images: ' + str(N_PATCHES_PER_IMAGE))\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "\n",
    "    data = np.array([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_images(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    tmp_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            img = Image.open(image_filename)\n",
    "            tmp_imgs.append(np.asarray(img) / 255)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    return tmp_imgs\n",
    "\n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return 1 #return [0, 1]\n",
    "    else:\n",
    "        return 0 #return [1, 0]\n",
    "\n",
    "def enhance(train_data, train_labels):\n",
    "    data_copy = list(train_data)\n",
    "    data_copy += [np.rot90(im) for im in train_data]\n",
    "    data_copy += [np.rot90(im,k=2) for im in train_data]\n",
    "    data_copy += [np.rot90(im,k=3) for im in train_data]\n",
    "    data_copy += [np.fliplr(im) for im in train_data]\n",
    "    data_copy += [np.flipud(im) for im in train_data]\n",
    "    data_copy += [np.transpose(im, (1,0,2)) for im in train_data]\n",
    "    return np.array(train_data), np.tile(train_labels)\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    tmp_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = Image.open(image_filename)\n",
    "            tmp_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(tmp_imgs)\n",
    "    for i in range(num_images):\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i]) / 255)        \n",
    "        '''gt_imgs.append(np.asarray(tmp_imgs[i].rotate(90)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].rotate(180)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].rotate(-90)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_LEFT_RIGHT)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_TOP_BOTTOM)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].transpose(Image.TRANSPOSE)) / 255)'''\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.array([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.array([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)\n",
    "\n",
    "# Extract label images\n",
    "def extract_gtim(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    tmp_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = Image.open(image_filename)\n",
    "            tmp_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return tmp_imgs\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img_win(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx] > 0.5:\n",
    "                array_labels[i:i+w, j:j+h] = 1\n",
    "            else:\n",
    "                array_labels[i:i+w, j:j+h] = 0\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx] > 0.5:\n",
    "                array_labels[j:j+w, i:i+h] = 1\n",
    "            else:\n",
    "                array_labels[j:j+w, i:i+h] = 0\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img_soft(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            array_labels[j:j+w, i:i+h] = labels[idx]\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Generate windows with mirror boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def compute(img_train, train_labels, window_size, patch_size):\n",
    "    train_data = []\n",
    "    nb_windows = (IMG_SIZE/patch_size)**2\n",
    "    for im in img_train:\n",
    "        w_im = sliding_window(im, window_size, patch_size)\n",
    "        train_data += w_im\n",
    "    \n",
    "    return np.asarray(train_data).reshape((-1, window_size, window_size, 3)), np.asarray(train_labels)\n",
    "\n",
    "def compute_test(imgs, window_size, patch_size):\n",
    "    res_data = []\n",
    "    nb_windows = (IMG_SIZE/patch_size)**2\n",
    "    for im in imgs:\n",
    "        w_im = sliding_window(im, window_size, patch_size)\n",
    "        res_data += w_im\n",
    "    \n",
    "    return np.asarray(res_data).reshape((-1, window_size, window_size, 3))\n",
    "\n",
    "def apply_mirror_boundary_conditions(coord, dim):\n",
    "    \"\"\"\n",
    "    Return the correct coordinate according to mirror boundary conditions\n",
    "        coord: a coordinate (x or y) in the image\n",
    "        dim: the length of the axis of said coordinate\n",
    "    \"\"\"\n",
    "    # If the coordinate is outside of the bounds of the axis, take its reflection inside the image\n",
    "    if coord < 0:\n",
    "        coord = -coord\n",
    "    elif coord >= dim:\n",
    "        coord =  2*(dim-1) - coord % (2*(dim-1))\n",
    "    # Else, do nothing\n",
    "    return int(coord)\n",
    "\n",
    "def get_window(image, window_size, corner_coordinates, patch_size):\n",
    "    \"\"\"\n",
    "    Get a window in image, centered on a patch, taking into account boundary conditions\n",
    "        image: a numpy array representing our image\n",
    "        window_size: an even number specifying the size of the window\n",
    "        corner_coordinates: a list containing the x-y coordinates of the patch's upleft pixel\n",
    "        path_size: an even number specifying the size of the central patch\n",
    "    \"\"\"\n",
    "    # Get convenient variables\n",
    "    window_radius = window_size/2\n",
    "    border_size = (window_size - patch_size)/2\n",
    "    i_patch_corner, j_patch_corner = (corner_coordinates[0], corner_coordinates[1])\n",
    "    i_window_corner, j_window_corner = (i_patch_corner - border_size, j_patch_corner - border_size)\n",
    "    nrows, ncols, nchannels = image.shape\n",
    "    window = np.zeros((window_size, window_size, nchannels))\n",
    "    \n",
    "    # Fill in the window array with pixels of the image\n",
    "    for i in range(window_size):\n",
    "        # Apply mirror boundary conditions on the x-coordinate\n",
    "        i_mirrored = apply_mirror_boundary_conditions(i_window_corner + i, nrows)\n",
    "        for j in range(window_size):\n",
    "            # Same for the y-coordinate\n",
    "            j_mirrored = apply_mirror_boundary_conditions(j_window_corner + j, ncols)\n",
    "            # Fill in the window with the corresponding pixel\n",
    "            window[i, j, :] = image[i_mirrored, j_mirrored, :]\n",
    "    return window\n",
    "def shift_to_the_right(image, window, corner_coordinates, patch_size):\n",
    "    nrows, ncols, _ = image.shape\n",
    "    window_size = len(window)\n",
    "    #window_radius = window_size/2\n",
    "    border_size = (window_size - patch_size)/2\n",
    "    i_patch_corner, j_patch_corner = (corner_coordinates[0], corner_coordinates[1])\n",
    "    i_window_corner, j_window_corner = (i_patch_corner - border_size, j_patch_corner - border_size)\n",
    "    step = patch_size\n",
    "    \n",
    "    shifted = np.roll(window, -step, axis=1)\n",
    "    for i in range(window_size):\n",
    "        i_mirrored = apply_mirror_boundary_conditions(i_window_corner + i, nrows)            \n",
    "        for j in range(window_size-step, window_size):\n",
    "            j_mirrored = apply_mirror_boundary_conditions(j_window_corner + j + step, ncols)\n",
    "            shifted[i, j, :] = image[i_mirrored, j_mirrored, :]\n",
    "    return shifted\n",
    "\n",
    "def shift_to_the_bottom(image, window, corner_coordinates, patch_size):\n",
    "    nrows, ncols, _ = image.shape\n",
    "    window_size = len(window)\n",
    "    #window_radius = window_size/2\n",
    "    border_size = (window_size - patch_size)/2\n",
    "    i_patch_corner, j_patch_corner = (corner_coordinates[0], corner_coordinates[1])\n",
    "    i_window_corner, j_window_corner = (i_patch_corner - border_size, j_patch_corner - border_size)\n",
    "    step = patch_size\n",
    "    \n",
    "    shifted = np.roll(window, -step, axis=0)\n",
    "    for j in range(window_size):\n",
    "        j_mirrored = apply_mirror_boundary_conditions(j_window_corner + j, ncols)\n",
    "        for i in range(window_size-step, window_size):\n",
    "            i_mirrored = apply_mirror_boundary_conditions(i_window_corner + i + step, nrows)\n",
    "            shifted[i, j, :] = image[i_mirrored, j_mirrored, :]\n",
    "    return shifted\n",
    "\n",
    "def sliding_window(image, window_size, patch_size):\n",
    "    \"\"\"\n",
    "    Construct a list of sliding windows of given size on an image.\n",
    "    The windows, centered on a patch, will slide from left to right and from up to down.\n",
    "        image: a numpy array representing our image\n",
    "        window_size: an even number specifying the size of the window\n",
    "        patch_size: the size of the central patch\n",
    "    \"\"\"\n",
    "    nrows, ncols, _ = image.shape\n",
    "    windows = []\n",
    "    i = 0\n",
    "    row_windows = [get_window(image, window_size, [i, 0], patch_size)]\n",
    "    for j in range(patch_size, ncols-1, patch_size):\n",
    "        #print(j)\n",
    "        row_windows += [shift_to_the_right(image, row_windows[-1], [i, j], patch_size)]\n",
    "    windows += row_windows\n",
    "    #print('===')\n",
    "    for i in range(patch_size, nrows-1, patch_size):\n",
    "        #print(i)\n",
    "        row_windows = [shift_to_the_bottom(image, row_windows[int(j/patch_size)], [i, j], patch_size) \n",
    "                       for j in range(0, ncols-1, patch_size)]\n",
    "        #print(len(row_windows))\n",
    "        windows += row_windows\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load images from dataset, convert to inputs for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATCH_SIZE = 16\n",
    "TRAINING_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "a = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches per images: 625.0\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, TRAINING_SIZE)\n",
    "train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62500, 16, 16, 3), (62500,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size = 0.25)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "print('Image size = ' + str(train_data[0].shape[0]) + ',' + str(train_data[0].shape[1]))\n",
    "\n",
    "# Show first image and its groundtruth image\n",
    "cimg = label_to_img(400, 400, 16, 16, train_labels[:,])\n",
    "fig1 = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cimg, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_imgs = extract_images(train_data_filename, 100)\n",
    "train_labels = extract_labels(train_labels_filename, 100)\n",
    "\n",
    "imgs, labels = compute(train_imgs, train_labels, 32, PATCH_SIZE)\n",
    "imgs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tr, img_t, labels_tr, labels_t = train_test_split(imgs, train_labels, test_size=0.1)\n",
    "img_tr.shape, img_t.shape, labels_tr.shape, labels_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ConvNet-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple VGG-like ConvNet (gives ~84% acc, maybe longer training gives extra 2-3%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, 2, input_shape=(INPUT_SIZE, INPUT_SIZE, NUM_CHANNELS)))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Conv2D(16, 2))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, 2))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Conv2D(32, 2))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, 1))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Conv2D(64, 1))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FCN (again, from tutorial network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, 2, input_shape=(INPUT_SIZE, INPUT_SIZE, NUM_CHANNELS)))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Conv2D(32, 2))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, 2))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(Conv2D(64, 2))\n",
    "model.add(LeakyReLU(alpha=a))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2DTranspose(1, kernel_size=1, padding='valid'))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.input_shape,layer.output_shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "DECAY = 0.00000\n",
    "adam = Adam(lr=LR, decay=DECAY)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc',f1])\n",
    "\n",
    "model.fit(X_train, 1-y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(X_test, 1-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(imgs, labels)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FCN_W16_noenh.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_test_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    names = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"test_{}/test_{}\".format(i,i)\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "            names.append(image_filename)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "    return np.array(imgs), names\n",
    "\n",
    "# Get prediction for given input image \n",
    "def keras_prediction(model, img):\n",
    "    data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    output_prediction = model.predict(data)\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "# Get prediction for given input image \n",
    "def keras_prediction_win(model, img):\n",
    "    img_wins = np.asarray(sliding_window(img, WINDOW_SIZE, IMG_PATCH_SIZE))\n",
    "    output_prediction = model.predict(img_wins)\n",
    "    img_prediction = label_to_img_win(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def mask_to_submission_strings(im, im_name):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", im_name).group(0))\n",
    "    for j in range(0, im.shape[1], IMG_PATCH_SIZE):\n",
    "        for i in range(0, im.shape[0], IMG_PATCH_SIZE):\n",
    "            patch = im[i:i + IMG_PATCH_SIZE, j:j + IMG_PATCH_SIZE]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, images, images_names):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for ind, fn in enumerate(images):\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn, images_names[ind]))\n",
    "# Get prediction for given input image \n",
    "def keras_prediction_av(model1, model2, img):\n",
    "    data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    pred1 = model1.predict(data)\n",
    "    pred2 = 1 - model2.predict(data)\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, (pred1 + pred2) / 2)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "# Get prediction for given input image \n",
    "def keras_prediction_av3(model1, model2, model3, img):\n",
    "    data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    pred1 = model1.predict(data)\n",
    "    pred2 = 1 - model2.predict(data)\n",
    "    pred3 = 1 - model3.predict(data)\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, (pred1 + pred2 + pred3) / 3)\n",
    "\n",
    "    return img_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 50\n",
    "test_data_dir = 'test_set_images/'\n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "test_data, file_names = extract_test_data(test_data_dir, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 608, 608, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model('FCN_16_enh.h5', custom_objects= {'f1_score': f1})\n",
    "model2 = load_model('DilNet_16_enh.h5', custom_objects= {'f1': f1})\n",
    "model3 = load_model('FCN_16_enh2.h5', custom_objects= {'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bcdfa62f98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bcdfaf07f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(keras_prediction_av3(model1,model2,model3, test_data[0]), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bce7a194a8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYWZZDggzy3JAmFnW/wGvUx/S7lTnbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bcdfbbd5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(keras_prediction_av(model1,model2, test_data[0]), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = 1-model1.predict(train_data)\n",
    "pred2 = model2.predict(train_data)\n",
    "pred3 = model3.predict(train_data)\n",
    "res = (pred1 + pred2 + pred3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.90583938541622944,\n",
       " 0.91367007135861211,\n",
       " 0.89681325111401178,\n",
       " 0.91836886596841949,\n",
       " 0.92067530563401534,\n",
       " 0.9140984596147419)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1_l = [1 if i > 0.5 else 0 for i in pred1]\n",
    "pred2_l = [1 if i > 0.5 else 0 for i in pred2]\n",
    "pred3_l = [1 if i > 0.5 else 0 for i in pred3]\n",
    "res_l = [1 if i > 0.5 else 0 for i in (pred1 + pred2 + pred3) / 3]\n",
    "res2_l = [1 if i > 0.5 else 0 for i in (pred1 + pred2) / 2]\n",
    "res_ll = [1 if i > 0.5 else 0 for i in (pred1 + pred3) / 2]\n",
    "f1_score(1-train_labels, pred1_l), f1_score(1-train_labels, pred2_l), f1_score(1-train_labels, pred3_l), f1_score(1-train_labels, res2_l), f1_score(1-train_labels, res_l), f1_score(1-train_labels, res_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = numpy.asarray(img_crop(test_data[0], IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "output_prediction = model.predict(tmp_data)\n",
    "img = label_to_img(608, 608, 16, 16, output_prediction)\n",
    "img_s = label_to_img_soft(608, 608, 16, 16, output_prediction)\n",
    "plt.imshow(img, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = np.asarray(sliding_window(test_data[0], WINDOW_SIZE, IMG_PATCH_SIZE))\n",
    "output_prediction = model.predict(img_1)\n",
    "plt.imshow(label_to_img(608, 608, 16, 16, output_prediction), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filename = 'silly_submission2.csv'\n",
    "images = [keras_prediction_av(model1, model2, test_data[i]) for i in range(test_data.shape[0])]\n",
    "masks_to_submission(submission_filename, images, file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 6-dimensional features consisting of average RGB color as well as variance\n",
    "def extract_features(img):\n",
    "    feat_m = np.mean(img, axis=(0,1))\n",
    "    feat_v = np.var(img, axis=(0,1))\n",
    "    feat = np.append(feat_m, feat_v)\n",
    "    return feat\n",
    "\n",
    "# Extract 2-dimensional features consisting of average gray color as well as variance\n",
    "def extract_features_2d(img):\n",
    "    feat_m = np.mean(img)\n",
    "    feat_v = np.var(img)\n",
    "    feat = np.append(feat_m, feat_v)\n",
    "    return feat\n",
    "\n",
    "# Extract features for a given image\n",
    "def extract_img_features(filename):\n",
    "    img = load_image(filename)\n",
    "    img_patches = img_crop(img, patch_size, patch_size)\n",
    "    X = np.asarray([ extract_features_2d(img_patches[i]) for i in range(len(img_patches))])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X_tr = np.asarray([ extract_features(X_train[i]) for i in range(len(X_train))])\n",
    "X_tr.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create an instance of the classifier and fit the data\n",
    "logreg = linear_model.LogisticRegression(C=1e5, class_weight=\"balanced\")\n",
    "logreg.fit(X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(1-y_train, 1-logreg.predict(X_tr)), f1_score(1-y_test, 1-logreg.predict(np.asarray([ extract_features(X_test[i]) for i in range(len(X_test))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([ extract_features(train_data[i]) for i in range(len(train_data))])\n",
    "X.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 5\n",
    "labels_f1 = 1 - train_labels\n",
    "res_tr = []\n",
    "res_t = []\n",
    "for i in range(n_trials):\n",
    "    tr_cur, t_cur, y_tr, y_t = train_test_split(X, labels_f1, test_size = 0.25)\n",
    "    logreg.fit(tr_cur, y_tr)\n",
    "    res_tr.append(f1_score(y_tr, logreg.predict(tr_cur)))\n",
    "    res_t.append(f1_score(y_t, logreg.predict(t_cur)))\n",
    "print(\"F1-score train: %0.3f +/- %0.3f\" % (np.array(res_tr).mean(), np.array(res_tr).std() * 2))\n",
    "print(\"F1-score test: %0.3f +/- %0.3f\" % (np.array(res_t).mean(), np.array(res_t).std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(32, 32, 64))\n",
    "mlp.fit(X_train.reshape((X_train.shape[0], -1)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(1-y_train, 1-mlp.predict(X_train.reshape((X_train.shape[0], -1)))), f1_score(1-y_test, 1-mlp.predict(X_test.reshape((X_test.shape[0], -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 5\n",
    "labels_f1 = 1 - train_labels\n",
    "res_tr = []\n",
    "res_t = []\n",
    "for i in range(n_trials):\n",
    "    tr_cur, t_cur, y_tr, y_t = train_test_split(train_data, labels_f1, test_size = 0.25)\n",
    "    mlp.fit(tr_cur.reshape((tr_cur.shape[0], -1)), y_tr)\n",
    "    res_tr.append(f1_score(y_tr, mlp.predict(tr_cur.reshape((tr_cur.shape[0], -1)))))\n",
    "    res_t.append(f1_score(y_t, mlp.predict(t_cur.reshape((t_cur.shape[0], -1)))))\n",
    "print(\"F1-score train: %0.3f +/- %0.3f\" % (np.array(res_tr).mean(), np.array(res_tr).std() * 2))\n",
    "print(\"F1-score test: %0.3f +/- %0.3f\" % (np.array(res_t).mean(), np.array(res_t).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(mlp, 'sklearn_mlp.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = numpy.asarray(img_crop(test_data[8], IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "pred = mlp.predict(test_img.reshape(test_img.shape[0],-1))\n",
    "plt.imshow(label_to_img(608, 608, 16, 16, pred), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_tr = np.asarray([ extract_features(X_train[i]) for i in range(len(X_train))])\n",
    "X_tr.shape, y_train.shape\n",
    "svc = SVC()\n",
    "svc.fit(X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(1-y_train, 1-svc.predict(X_tr)), f1_score(1-y_test, 1-svc.predict(np.asarray([ extract_features(X_test[i]) for i in range(len(X_test))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 5\n",
    "labels_f1 = 1 - train_labels\n",
    "res_tr = []\n",
    "res_t = []\n",
    "for i in range(n_trials):\n",
    "    tr_cur, t_cur, y_tr, y_t = train_test_split(X, labels_f1, test_size = 0.25)\n",
    "    svc.fit(tr_cur, y_tr)\n",
    "    res_tr.append(f1_score(y_tr, svc.predict(tr_cur)))\n",
    "    res_t.append(f1_score(y_t, svc.predict(t_cur)))\n",
    "print(\"F1-score train: %0.3f +/- %0.3f\" % (np.array(res_tr).mean(), np.array(res_tr).std() * 2))\n",
    "print(\"F1-score test: %0.3f +/- %0.3f\" % (np.array(res_t).mean(), np.array(res_t).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "rfc = ExtraTreesClassifier(n_estimators = 10)\n",
    "rfc.fit(X_train.reshape((X_train.shape[0], -1)), y_train)\n",
    "f1_score(1-y_train, 1-rfc.predict(X_train.reshape((X_train.shape[0], -1)))), f1_score(1-y_test, 1-rfc.predict(X_test.reshape((X_test.shape[0], -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 5\n",
    "labels_f1 = 1 - train_labels\n",
    "res_tr = []\n",
    "res_t = []\n",
    "for i in range(n_trials):\n",
    "    tr_cur, t_cur, y_tr, y_t = train_test_split(train_data, labels_f1, test_size = 0.25)\n",
    "    rfc.fit(tr_cur.reshape((tr_cur.shape[0], -1)), y_tr)\n",
    "    res_tr.append(f1_score(y_tr, rfc.predict(tr_cur.reshape((tr_cur.shape[0], -1)))))\n",
    "    res_t.append(f1_score(y_t, rfc.predict(t_cur.reshape((t_cur.shape[0], -1)))))\n",
    "print(\"F1-score train: %0.3f +/- %0.3f\" % (np.array(res_tr).mean(), np.array(res_tr).std() * 2))\n",
    "print(\"F1-score test: %0.3f +/- %0.3f\" % (np.array(res_t).mean(), np.array(res_t).std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.001, loss='exponential')\n",
    "gbc.fit(X_train.reshape((X_train.shape[0], -1)), y_train)\n",
    "f1_score(1-y_train, 1-gbc.predict(X_train.reshape((X_train.shape[0], -1)))), f1_score(1-y_test, 1-gbc.predict(X_test.reshape((X_test.shape[0], -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "model1 = load_model('FCN_16_enh.h5', custom_objects= {'f1_score': f1})\n",
    "model2 = load_model('DilNet_16_enh.h5', custom_objects= {'f1': f1})\n",
    "model3 = load_model('FCN_16_enh2.h5', custom_objects= {'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model1, to_file='FCN1.png',show_shapes =True,show_layer_names=False)\n",
    "plot_model(model2, to_file='DilNet1.png',show_shapes =True,show_layer_names=False)\n",
    "plot_model(model3, to_file='FCN2.png',show_shapes =True,show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
