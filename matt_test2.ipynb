{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os,sys\n",
    "import gc\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import timeit\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 400, 3) (100, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "# Directory and files name\n",
    "train_dir = \"training/\"\n",
    "tr_image_dir = train_dir + \"images/\"\n",
    "tr_label_dir = train_dir + \"groundtruth/\"\n",
    "\n",
    "tr_image_files = os.listdir(tr_image_dir)\n",
    "tr_label_files = os.listdir(tr_label_dir)\n",
    "\n",
    "# Number of training samples\n",
    "N = len(tr_image_files)\n",
    "\n",
    "# Load the images and ground truth\n",
    "img_train = []\n",
    "label_train = []\n",
    "for i in range(N):\n",
    "    img = mpimg.imread(tr_image_dir + tr_image_files[i])\n",
    "    label = mpimg.imread(tr_label_dir + tr_label_files[i])\n",
    "    \n",
    "    img_train.append(img)\n",
    "    label_train.append(label)\n",
    "\n",
    "# Keep only sub-set of images\n",
    "NUM_IMAGES = N\n",
    "\n",
    "img_train = np.asarray(img_train[:NUM_IMAGES])\n",
    "label_train = np.asarray(label_train[:NUM_IMAGES])\n",
    "\n",
    "print(img_train.shape, label_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    tmp_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = Image.open(image_filename)\n",
    "            tmp_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(tmp_imgs)\n",
    "    for i in range(num_images):\n",
    "        imgs.append(np.asarray(tmp_imgs[i]) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].rotate(90)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].rotate(180)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].rotate(-90)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_LEFT_RIGHT)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_TOP_BOTTOM)) / 255)\n",
    "        imgs.append(np.asarray(tmp_imgs[i].transpose(Image.TRANSPOSE)) / 255)\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "    print('Patches per images: ' + str(N_PATCHES_PER_IMAGE))\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "\n",
    "    data = np.array([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = np.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return 1 #return [0, 1]\n",
    "    else:\n",
    "        return 0 #return [1, 0]\n",
    "\n",
    "def enhance_save(train_data, train_labels):\n",
    "    data_copy = list(train_data)\n",
    "    path = './windows_train_patch/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for ind, im in enumerate(data_copy):\n",
    "        cur_name = path + str(ind)\n",
    "        plt.imsave(cur_name + \"_\" + str(train_labels[ind]) + \".png\", im)\n",
    "        plt.imsave(cur_name + \"_\" + str(train_labels[ind]) + \"r90.png\", np.rot90(im))\n",
    "        plt.imsave(cur_name + \"_\" + str(train_labels[ind]) + \"r180.png\", np.rot90(im,k=2))        \n",
    "        plt.imsave(cur_name + \"_\" + str(train_labels[ind]) + \"r270.png\", np.rot90(im,k=3))\n",
    "        plt.imsave(cur_name + \"_\" + str(train_labels[ind]) + \"flr.png\", np.fliplr(im))\n",
    "        plt.imsave(cur_name + \"_\" + str(train_labels[ind]) + \"fud.png\", np.flipud(im))\n",
    "        plt.imsave(cur_name + \"_\" + str(train_labels[ind]) + \"tr.png\", np.transpose(im,(1, 0, 2)))\n",
    "    \n",
    "    del data_copy\n",
    "\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    tmp_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = Image.open(image_filename)\n",
    "            tmp_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(tmp_imgs)\n",
    "    for i in range(num_images):\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i]) / 255)        \n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].rotate(90)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].rotate(180)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].rotate(-90)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_LEFT_RIGHT)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].transpose(Image.FLIP_TOP_BOTTOM)) / 255)\n",
    "        gt_imgs.append(np.asarray(tmp_imgs[i].transpose(Image.TRANSPOSE)) / 255)\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = np.array([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = np.array([value_to_class(np.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(np.float32)\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img_win(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx] > 0.5:\n",
    "                array_labels[i:i+w, j:j+h] = 1\n",
    "            else:\n",
    "                array_labels[i:i+w, j:j+h] = 0\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx] > 0.5:\n",
    "                array_labels[j:j+w, i:i+h] = 1\n",
    "            else:\n",
    "                array_labels[j:j+w, i:i+h] = 0\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img_soft(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            array_labels[j:j+w, i:i+h] = labels[idx]\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * PIXEL_DEPTH).round().astype(np.uint8)\n",
    "    return rimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features for each image patch\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "def value_to_class(v):\n",
    "    df = np.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "label_train = [value_to_class(np.mean(label_train[i])) for i in range(len(label_train))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance the training dataset w/ sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 400\n",
    "WINDOW_SIZE = 52 # 18px - PATCH_SIZE - 18px\n",
    "NB_WINDOWS = (IMG_SIZE/PATCH_SIZE)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mirror_boundary_conditions(coord, dim):\n",
    "    \"\"\"\n",
    "    Return the correct coordinate according to mirror boundary conditions\n",
    "        coord: a coordinate (x or y) in the image\n",
    "        dim: the length of the axis of said coordinate\n",
    "    \"\"\"\n",
    "    # If the coordinate is outside of the bounds of the axis, take its reflection inside the image\n",
    "    if coord < 0:\n",
    "        coord = -coord\n",
    "    elif coord >= dim:\n",
    "        coord =  2*(dim-1) - coord % (2*(dim-1))\n",
    "    # Else, do nothing\n",
    "    return int(coord)\n",
    "\n",
    "def get_window(image, window_size, corner_coordinates, patch_size):\n",
    "    \"\"\"\n",
    "    Get a window in image, centered on a patch, taking into account boundary conditions\n",
    "        image: a numpy array representing our image\n",
    "        window_size: an even number specifying the size of the window\n",
    "        corner_coordinates: a list containing the x-y coordinates of the patch's upleft pixel\n",
    "        path_size: an even number specifying the size of the central patch\n",
    "    \"\"\"\n",
    "    # Get convenient variables\n",
    "    window_radius = window_size/2\n",
    "    border_size = (window_size - patch_size)/2\n",
    "    i_patch_corner, j_patch_corner = (corner_coordinates[0], corner_coordinates[1])\n",
    "    i_window_corner, j_window_corner = (i_patch_corner - border_size, j_patch_corner - border_size)\n",
    "    nrows, ncols, nchannels = image.shape\n",
    "    window = np.zeros((window_size, window_size, nchannels))\n",
    "    \n",
    "    # Fill in the window array with pixels of the image\n",
    "    for i in range(window_size):\n",
    "        # Apply mirror boundary conditions on the x-coordinate\n",
    "        i_mirrored = apply_mirror_boundary_conditions(i_window_corner + i, nrows)\n",
    "        for j in range(window_size):\n",
    "            # Same for the y-coordinate\n",
    "            j_mirrored = apply_mirror_boundary_conditions(j_window_corner + j, ncols)\n",
    "            # Fill in the window with the corresponding pixel\n",
    "            window[i, j, :] = image[i_mirrored, j_mirrored, :]\n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_to_the_right(image, window, corner_coordinates, patch_size):\n",
    "    nrows, ncols, _ = image.shape\n",
    "    window_size = len(window)\n",
    "    #window_radius = window_size/2\n",
    "    border_size = (window_size - patch_size)/2\n",
    "    i_patch_corner, j_patch_corner = (corner_coordinates[0], corner_coordinates[1])\n",
    "    i_window_corner, j_window_corner = (i_patch_corner - border_size, j_patch_corner - border_size)\n",
    "    step = patch_size\n",
    "    \n",
    "    shifted = np.roll(window, -step, axis=1)\n",
    "    for i in range(window_size):\n",
    "        i_mirrored = apply_mirror_boundary_conditions(i_window_corner + i, nrows)            \n",
    "        for j in range(window_size-step, window_size):\n",
    "            j_mirrored = apply_mirror_boundary_conditions(j_window_corner + j + step, ncols)\n",
    "            shifted[i, j, :] = image[i_mirrored, j_mirrored, :]\n",
    "    return shifted\n",
    "\n",
    "def shift_to_the_bottom(image, window, corner_coordinates, patch_size):\n",
    "    nrows, ncols, _ = image.shape\n",
    "    window_size = len(window)\n",
    "    #window_radius = window_size/2\n",
    "    border_size = (window_size - patch_size)/2\n",
    "    i_patch_corner, j_patch_corner = (corner_coordinates[0], corner_coordinates[1])\n",
    "    i_window_corner, j_window_corner = (i_patch_corner - border_size, j_patch_corner - border_size)\n",
    "    step = patch_size\n",
    "    \n",
    "    shifted = np.roll(window, -step, axis=0)\n",
    "    for j in range(window_size):\n",
    "        j_mirrored = apply_mirror_boundary_conditions(j_window_corner + j, ncols)\n",
    "        for i in range(window_size-step, window_size):\n",
    "            i_mirrored = apply_mirror_boundary_conditions(i_window_corner + i + step, nrows)\n",
    "            shifted[i, j, :] = image[i_mirrored, j_mirrored, :]\n",
    "    return shifted\n",
    "\n",
    "def sliding_window(image, window_size, patch_size):\n",
    "    \"\"\"\n",
    "    Construct a list of sliding windows of given size on an image.\n",
    "    The windows, centered on a patch, will slide from left to right and from up to down.\n",
    "        image: a numpy array representing our image\n",
    "        window_size: an even number specifying the size of the window\n",
    "        patch_size: the size of the central patch\n",
    "    \"\"\"\n",
    "    nrows, ncols, _ = image.shape\n",
    "    windows = []\n",
    "    i = 0\n",
    "    row_windows = [get_window(image, window_size, [i, 0], patch_size)]\n",
    "    for j in range(patch_size, ncols-1, patch_size):\n",
    "        #print(j)\n",
    "        row_windows += [shift_to_the_right(image, row_windows[-1], [i, j], patch_size)]\n",
    "    windows += row_windows\n",
    "    #print('===')\n",
    "    for i in range(patch_size, nrows-1, patch_size):\n",
    "        #print(i)\n",
    "        row_windows = [shift_to_the_bottom(image, row_windows[int(j/patch_size)], [i, j], patch_size) \n",
    "                       for j in range(0, ncols-1, patch_size)]\n",
    "        #print(len(row_windows))\n",
    "        windows += row_windows\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance(train_data):\n",
    "    #print(len(train_data), train_data[0].shape)\n",
    "    data_copy = list(train_data)\n",
    "    \n",
    "    train_data += [np.rot90(im) for im in data_copy]\n",
    "    train_data += [np.rot90(im, k=2) for im in data_copy]\n",
    "    train_data += [np.rot90(im, k=3) for im in data_copy]\n",
    "    train_data += [np.fliplr(im) for im in data_copy]\n",
    "    train_data += [np.flipud(im) for im in data_copy]\n",
    "    train_data += [np.transpose(im, (1,0,2)) for im in data_copy]\n",
    "    \n",
    "    del data_copy\n",
    "    gc.collect()\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(img_train, train_labels, window_size, patch_size):\n",
    "    train_data = []\n",
    "    nb_windows = (IMG_SIZE/patch_size)**2\n",
    "    i = 0\n",
    "    for im in img_train:\n",
    "        print(i)\n",
    "        #w_im = enhance(sliding_window(im, window_size, patch_size))\n",
    "        w_im = sliding_window(im, window_size, patch_size)\n",
    "        #w_labels = sliding_window(labels[:, :, np.newaxis], window_size, step)\n",
    "        train_data += w_im\n",
    "        i += 1\n",
    "        #train_labels += w_labels\n",
    "        #path = './windows_train_patch/' + str(i)\n",
    "        #os.makedirs(path, exist_ok=True)\n",
    "        #for wi, j in zip(w_im, range(len(w_im))):\n",
    "            #img_name = path + '/im_' + str(j) + '.png'\n",
    "            #plt.imsave(img_name, wi)\n",
    "            #label_name = path + '/label_' + str(j) + '.png'\n",
    "            #plt.imsave(label_name, wl[:,:,0])\n",
    "    #train_labels *= 7\n",
    "    \n",
    "    return np.asarray(train_data).reshape((len(train_labels), window_size, window_size, 3)), np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_train, label_train = compute(img_train, label_train, WINDOW_SIZE, PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#a = np.array(range(10*10))\n",
    "#a = a.reshape((10, 10))\n",
    "#print(a)\n",
    "#aa = get_window(a[:,:,np.newaxis], 5, [5,5])[:,:,0]\n",
    "#print('\\n')\n",
    "#print(aa)\n",
    "#print('\\n')\n",
    "#print(shift_to_the_bottom(a[:,:,np.newaxis], aa[:,:,np.newaxis], [5,5], 2)[:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "#IMG_PATCH_SIZE = 52\n",
    "#TRAINING_SIZE = 62500\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 15\n",
    "a = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LeakyReLU, Input, Reshape, Permute, Average\n",
    "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Lambda, Activation, GlobalAveragePooling2D, Conv2DTranspose, GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model0(IMG_PATCH_SIZE, TRAINING_SIZE):\n",
    "    model0 = Sequential()\n",
    "    model0.add(Conv2D(32, 2, input_shape=(IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS)))\n",
    "    model0.add(LeakyReLU(alpha=a))\n",
    "    model0.add(Conv2D(32, 2))\n",
    "    model0.add(LeakyReLU(alpha=a))\n",
    "    model0.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model0.add(Dropout(0.25))\n",
    "\n",
    "    model0.add(Conv2D(64, 2))\n",
    "    model0.add(LeakyReLU(alpha=a))\n",
    "    model0.add(Conv2D(64, 2))\n",
    "    model0.add(LeakyReLU(alpha=a))\n",
    "    model0.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model0.add(Dropout(0.25))\n",
    "\n",
    "    model0.add(Flatten())\n",
    "    model0.add(Dense(128))\n",
    "    model0.add(Dropout(0.5))\n",
    "    model0.add(LeakyReLU(alpha=a))\n",
    "    model0.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "def fire(x, squeeze, expand):\n",
    "    x = Conv2D(squeeze, 1)(x)\n",
    "    x = LeakyReLU(alpha=a)(x)\n",
    "    e11 = Conv2D(expand, 1)(x)\n",
    "    e11 = LeakyReLU(alpha=a)(e11)\n",
    "    e33 = Conv2D(expand, 1)(x)\n",
    "    e33 = LeakyReLU(alpha=a)(e33)\n",
    "    return Concatenate(axis=3)([e11, e33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(IMG_PATCH_SIZE, TRAINING_SIZE):\n",
    "    inputs = Input(shape=(IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS))\n",
    "    x = Conv2D(32, kernel_size=3, strides=2, input_shape=(IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS))(inputs)\n",
    "    x = LeakyReLU(alpha=a)(x)\n",
    "    x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "\n",
    "    x = fire(x, squeeze=8, expand=16)\n",
    "    x = fire(x, squeeze=8, expand=16)\n",
    "    x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "\n",
    "    '''x = fire(x, squeeze=32, expand=128)\n",
    "    x = fire(x, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "\n",
    "    x = fire(x, squeeze=48, expand=192)\n",
    "    x = fire(x, squeeze=48, expand=192)\n",
    "    x = fire(x, squeeze=64, expand=256)\n",
    "    x = fire(x, squeeze=64, expand=256)\n",
    "    x = MaxPooling2D(pool_size=3, strides=2)(x)'''\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model1 = Model(inputs, x)\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(IMG_PATCH_SIZE, TRAINING_SIZE):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(32, 2, input_shape=(IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS)))\n",
    "    model2.add(LeakyReLU(alpha=a))\n",
    "    model2.add(Conv2D(32, 2))\n",
    "    model2.add(LeakyReLU(alpha=a))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "\n",
    "    model2.add(Conv2D(64, 2))\n",
    "    model2.add(LeakyReLU(alpha=a))\n",
    "    model2.add(Conv2D(64, 2))\n",
    "    model2.add(LeakyReLU(alpha=a))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "\n",
    "    model2.add(Conv2D(2, 1))\n",
    "    model2.add(Conv2DTranspose(1, kernel_size=IMG_PATCH_SIZE - 1, padding='valid'))\n",
    "    #model.add(Reshape((-1, IMG_PATCH_SIZE * IMG_PATCH_SIZE)))\n",
    "    #model.add(Permute((2,1)))\n",
    "    model2.add(GlobalAveragePooling2D())\n",
    "    model2.add(Activation('sigmoid'))\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in model.layers:\n",
    "    #print(layer.input_shape,layer.output_shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_window_size = np.nan\n",
    "best_model = np.nan\n",
    "best_score = 0\n",
    "\n",
    "LR = 0.001\n",
    "DECAY = 0.00000\n",
    "adam = Adam(lr=LR, decay=DECAY)\n",
    "\n",
    "for window_size in range(30, 60, 2):\n",
    "    imgs, labels = compute(img_train, label_train, window_size, PATCH_SIZE)\n",
    "    training_size = imgs.shape[0]\n",
    "    num_model = 0\n",
    "    for model in [model0(window_size, training_size)]:#, model1(window_size, training_size), model2(window_size, training_size)]:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1])\n",
    "        model.fit(imgs, labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "        score = model.evaluate(train, labels)[1]\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = num_model\n",
    "            best_window_size = window_size\n",
    "        num_model += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = compute(img_train, label_train, 16, PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MATT\\Anaconda3\\lib\\site-packages\\keras\\models.py:288: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "#path = 'FCN_W32_noenh.h5'\n",
    "path = 'FCN_16_enh.h5'\n",
    "model = keras.models.load_model(path,custom_objects={'f1_score':f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_predictions(predictions):\n",
    "    predictions[predictions < 0.5] = 0\n",
    "    predictions[predictions > 0] = 1\n",
    "    nb_predictions = int(len(predictions)/NUM_IMAGES)\n",
    "    pred = []\n",
    "    for i in range(0, len(predictions), k):\n",
    "        pred += [predictions[i:i+k].reshape((25, 25))]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def remove_small_pieces(predictions, thresh_size=4):\n",
    "    modified = []\n",
    "    for p in predictions:\n",
    "        # find connected components\n",
    "        labeled, nr_objects = ndimage.label(p) \n",
    "        #print(nr_objects)\n",
    "        areas = [len(np.where(labeled == i)[0]) for i in range(1, nr_objects+1)]\n",
    "        #print(areas)\n",
    "        smallest = np.where(np.array(areas) <= thresh_size)[0]\n",
    "        #print(smallest)\n",
    "        for i in smallest:\n",
    "            labeled[np.where(labeled == i+1)] = 0\n",
    "        modified += [labeled]\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_road(list_, length_road, max_hole):\n",
    "    N = len(list_)\n",
    "    roads = []\n",
    "    end = []\n",
    "    for i in range(N-length_road):\n",
    "        #print(np.all(list_[i:i+length_road] > 0))\n",
    "        roads += [np.all(list_[i:i+length_road] > 0)]\n",
    "    for i in range(N, 0, -1):\n",
    "        end += [np.all(list_[i-length_road:i] > 0)]\n",
    "    roads += end[:length_road]\n",
    "    roads = fill_holes(roads, max_hole)\n",
    "    return roads\n",
    "\n",
    "def fill_holes(list_, max_hole):\n",
    "    idx = np.where(np.array(list_))[0]\n",
    "    distances = idx[1:] - idx[0:-1]\n",
    "    tofill = np.where(distances <= max_hole)[0]\n",
    "    for i in tofill:\n",
    "        list_[i] = True\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_roads(predictions, length_road=5, max_hole=4):\n",
    "    modified = []\n",
    "    for p in predictions:\n",
    "        L = []\n",
    "        C = []\n",
    "        for i in range(len(p)):\n",
    "            L += [check_road(p[i, :], length_road, max_hole)]\n",
    "            C += [check_road(p[:, i], length_road, max_hole)]\n",
    "        L = np.asarray(L)\n",
    "        C = np.asarray(C).T\n",
    "        #print(L.shape, C.shape)\n",
    "        roads = (L+C).astype(int)\n",
    "        modified += [roads.T]\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_predictions(predictions):\n",
    "    return remove_small_pieces(detect_roads(patch_predictions(predictions))), detect_roads(remove_small_pieces(patch_predictions(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mod1, pred_mod2 = np.asarray(treat_predictions(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mod1[pred_mod1 > 0] = 1\n",
    "pred_mod2[pred_mod2 > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(patch_predictions(labels))\n",
    "pred = np.asarray(patch_predictions(predictions)).astype(int)\n",
    "pred = np.transpose(pred, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80986204025 0.839438421969 0.834165798626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_pred = 0\n",
    "f1_mod1 = 0\n",
    "f1_mod2 = 0\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    f1_pred += f1_score(1-y_train[i].ravel(), 1-pred[i].ravel())\n",
    "    f1_mod1 += f1_score(1-y_train[i].ravel(), 1-pred_mod1[i].ravel())\n",
    "    f1_mod2 += f1_score(1-y_train[i].ravel(), 1-pred_mod2[i].ravel())\n",
    "\n",
    "f1_pred /= y_train.shape[0]\n",
    "f1_mod1 /= y_train.shape[0]\n",
    "f1_mod2 /= y_train.shape[0]\n",
    "\n",
    "print(f1_pred, f1_mod1, f1_mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851358397402 (4, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "best_params = (0, 0, 0)\n",
    "\n",
    "for area in range(1, 5):\n",
    "    for road in range(1, 7):\n",
    "        for hole in range(1, 7):\n",
    "            e = np.asarray(remove_small_pieces(detect_roads(patch_predictions(predictions), road, hole), area))\n",
    "            e[e > 0] = 1\n",
    "            f1 = 0\n",
    "            for i in range(y_train.shape[0]):\n",
    "                f1 += f1_score(1-y_train[i].ravel(), 1-e[i].ravel())\n",
    "            f1 /= y_train.shape[0]\n",
    "            if f1 > score:\n",
    "                score = f1\n",
    "                best_params = (area, road, hole)\n",
    "\n",
    "print(score, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(remove_small_pieces(detect_roads(patch_predictions(predictions), best_params[1], best_params[2]), best_params[0]))\n",
    "a[a > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b9054039b0>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH0iK8ANJ/R+/kE5skQ5aUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b9053c0a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(a[77, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 608, 608, 3)\n"
     ]
    }
   ],
   "source": [
    "# Directory and files name\n",
    "test_dir = \"test_set_images/\"\n",
    "\n",
    "test_image_files = os.listdir(test_dir)\n",
    "\n",
    "# Number of testing samples\n",
    "N = len(test_image_files)\n",
    "\n",
    "# Load the images and ground truth\n",
    "img_test = []\n",
    "for i in range(N):\n",
    "    img = mpimg.imread(test_dir + test_image_files[i] + '/' + test_image_files[i] + '.png')\n",
    "    \n",
    "    img_test.append(img)\n",
    "\n",
    "img_test = np.asarray(img_test)\n",
    "\n",
    "print(img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs = np.asarray(img_crop(img_test[0], PATCH_SIZE, PATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(imgs)\n",
    "pred = label_to_img(608, 608, 16, 16, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(remove_small_pieces(detect_roads([pred], best_params[1]*2, best_params[2]*2), best_params[0]*2))\n",
    "a[a > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b90c719908>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b90c6844a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "kernel = np.ones((5,5))\n",
    "k2 = np.ones((20,20))\n",
    "k3 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
    "\n",
    "closing = cv2.morphologyEx(pred.astype(np.float32), cv2.MORPH_CLOSE, k2)\n",
    "erosion = cv2.erode(closing, kernel, iterations = 1)\n",
    "opening = cv2.morphologyEx(erosion, cv2.MORPH_OPEN, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(remove_small_pieces(detect_roads([erosion], best_params[1], best_params[2]), best_params[0]))\n",
    "a[a > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b90d5ac390>"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEnlJREFUeJzt3W2MXNV9x/HvL2vAKRCMjUHGNjUI\nqwVVxbgrMKJCKc4DkCjmBbSgKLjU1UotkYioREwrNa3UF9AXgaBEpBbQmigJUBKKhWgcZEBpX/Cw\nBmMeHMJCXbxaF4MBJy1KqN1/X9yzYVjP2b2ze+/cmfHvI63m3jNn5v53Z/Y35z6OIgIzs3Y+1nQB\nZta7HBBmluWAMLMsB4SZZTkgzCzLAWFmWbUEhKRLJL0iaUzSxjqWYWb1U9XHQUgaAn4GfBoYB54B\nro6IlytdkJnVro4RxHnAWES8HhEfAPcC62pYjpnVbF4Nz7kU2NMyPw6cP90DTlo4FCuWH1VDKWY2\nafvOX70dEYs7eUwdAaE2bYetx0gaAUYATls6j6e3Lq+hFDObNLRk7D87fUwdqxjjQOt/+zJgYmqn\niNgUEcMRMbx40VANZZjZXNUREM8AKyWdLulo4CpgSw3LMbOaVb6KEREHJX0Z2AoMAXdHxEtVL8fM\n6lfHNggi4hHgkTqe28y6x0dSmlmWA8LMshwQZpblgDCzLAeEmWU5IMwsywFhZlkOCDPLckCYWZYD\nwsyyHBBmluWAMLMsB4SZZTkgzCzLAWFmWQ4IM8tyQJhZVi1XlLLB99lTV3X8mK0TO2qoxOrkEYSZ\nZXkEMcB67VO+XT0eVfQ2jyDMLMsBYWZZXsVo2HSrAWWH3589dZWH6lYLB0QPy4WHw6BZ7QJ58rXK\ntbdT1+uYX+ZYx881MKsYM22Qm80Gu27YOrHjIz+zfQ6zOngEYT2nH1aZch84vfpBNFsDM4Iws+rN\nGBCS7pa0T9KLLW0LJT0q6dV0e2Jql6TbJY1J2ilpdZ3Fmx3JPnvqqtpHLGVWMf4J+CZwT0vbRmBb\nRNwsaWOa/ypwKbAy/ZwP3JFuKzHd0LPdxqHWtk42HpV5/qrM5gUepAOOuj0k76W/XT9shJ4xICLi\nJ5JWTGleB3wyTW8GnqAIiHXAPRERwJOSFkhaEhF7qyq4X1X5jzDXsOsl7f4Z6vx9eumfr5dqyZnt\nRspTJv/pI2KvpJNT+1JgT0u/8dR2WEBIGgFGAE5b6m2lvaqfw8fmrur/TLVpi3YdI2ITsAlg+Jz5\nbfvY4HMA9bbZ7sV4U9ISgHS7L7WPA8tb+i0DJmZfnpk1abYBsQVYn6bXAw+1tF+T9masAQ54+4NZ\n/5pxFUPS9yk2SJ4kaRz4GnAzcL+kDcAbwJWp+yPAZRTHdL4PXFtlsZ1u0Kpy+NoPW5wHSSev3Vxf\ngyYOh57O1HqafI+V2YtxdeautW36BnDdXIvqRJk/Xj8cmWcfdSS/XmV/97K7/CcNLem8Fh9Jadbj\nunFAVI73L/Yhb/kfXL022vUIwsyyPIIoyRsp7Uh0xASEh+U2G738vum8ts4vGNNXATHb9TN/yneX\n/971aCKs+iogzI4EvTRq8UZKM8tyQJhZ1hG9ilHFUM7r24fr5b9rLw3f+4FHEGaW1VcjiKo/Vbr5\n6d/ESKOKZeaew5/E7Q3S1b7AIwgzm0ZfjSCsef3+idgtg/J38gjCzLL6bgTRydGU3UjxXqqlCr2y\nDt3NC8ZUqV9e57L6LiA6MdMbp5un1m6d2DFwb55+VNVl9usK0jrfj75gjJlVygFhZlkDvYphNp2q\nVgsGedXRIwgzyxroEUSZZJ9r+g/aXoxB1+ujhnrfJ51fMMYjCDPLckCYWZYDwsyyHBBmluWAMLOs\nGQNC0nJJj0vaJeklSden9oWSHpX0aro9MbVL0u2SxiTtlLS67l/CzOpRZgRxEPiLiDgLWANcJ+ls\nYCOwLSJWAtvSPMClwMr0MwLcUXnVZtYVMwZEROyNiGfT9C+AXcBSYB2wOXXbDFyeptcB90ThSWCB\npFmcJmJmTetoG4SkFcC5wFPAKRGxF4oQAU5O3ZYCe1oeNp7apj7XiKRRSaNv7T/UeeVmVrvSASHp\nOOAHwFci4ufTdW3TFoc1RGyKiOGIGF68aKhsGWbWRaUCQtJRFOHw3Yj4YWp+c3LVId3uS+3jwPKW\nhy8DJqop18y6qcxeDAF3Absi4ustd20B1qfp9cBDLe3XpL0Za4ADk6siZtZfypysdSHwJeAFSZNn\nJv0lcDNwv6QNwBvAlem+R4DLKM4MeR+4ttKKOzR5MpVPljLr3IwBERH/TvvtCgBr2/QP4Lo51mVm\nPcBHUppZlgPCzLIcEGaW5YAwsywHhJllOSDMLMsBYWZZDggzy+qry97P5ot7fQSl2ez1VUB0os4v\nQXXozE4vfQu3leNVDDPLckCYWZYDwsyyBnYbhLcT9J66XhNv26iPRxBmltVXI4hOPila+7buHm1q\nZJGr3SMd62UeQZhZVl+NIAaJRw7WDzyCMLOsvhpBzOZQ69x8tzW9fLPZ8AjCzLL6KiC2Tuwo/Uk8\nOdLYOrHD+8nNZqmvAsLMussBYWZZDggzy+qrvRg50x0p6b0HZrM3Y0BImg/8BDgm9X8gIr4m6XTg\nXmAh8CzwpYj4QNIxwD3A7wH7gT+KiN011Q989DDm3CHWZThMzD6qzCrGr4CLI+IcYBVwSfrW7luA\nWyNiJfAusCH13wC8GxFnAremfmbWh2YMiCj8d5o9Kv0EcDHwQGrfDFyepteledL9ayXlvvzXzHpY\nqW0QkoaA7cCZwLeA14D3IuJg6jIOLE3TS4E9ABFxUNIBYBHwdoV1V8qrFmbtldqLERGHImIVsAw4\nDzirXbd02260EFMbJI1IGpU0+tb+Q2XrNbMu6mgvRkS8J+kJYA2wQNK8NIpYBkykbuPAcmBc0jzg\nBOCdNs+1CdgEMHzO/MMCpBPei2FWjxlHEJIWS1qQpj8OfArYBTwOXJG6rQceStNb0jzp/sciYk4B\nYGbNKDOCWAJsTtshPgbcHxEPS3oZuFfS3wHPAXel/ncB35E0RjFyuKqGus2sC2YMiIjYCZzbpv11\niu0RU9t/CVxZSXVm1igfam1mWQ4IM8tyQJhZ1kCcrFXmXAzv7jTrnEcQZpblgDCzLAeEmWU5IMws\nywFhZlkOCDPLckCYWZYDwsyyHBBmljUQR1L6gjFm9RiIgGg122//NutXZd/zQ0s6f26vYphZlgPC\nzLIcEGaW5YAws6yB20jpjY92pCn/nh/r+LkHIiByF4yZLYfMh3J/z7r/RlW8jjZ3XsUwsywHhJll\nDcQqhnWPV7+OLAMREL44bX2a+nt2slxvr6iPVzHMLKvvRhDtPi3K7MVoHWWYWTmlRxCShiQ9J+nh\nNH+6pKckvSrpPklHp/Zj0vxYun9FPaWbWd06WcW4HtjVMn8LcGtErATeBTak9g3AuxFxJnBr6mdm\nfahUQEhaBnwOuDPNC7gYeCB12QxcnqbXpXnS/WtTfzPrM2W3QdwG3Agcn+YXAe9FxME0Pw4sTdNL\ngT0AEXFQ0oHU/+0qCu5kW8LUvnVskfd2DRtkM44gJH0e2BcR21ub23SNEve1Pu+IpFFJo2/tP1Sq\nWDPrrjKrGBcCX5C0G7iXYtXiNmCBpMkRyDJgIk2PA8sB0v0nAO9MfdKI2BQRwxExvHjR0Jx+CTOr\nx4wBERE3RcSyiFgBXAU8FhFfBB4Hrkjd1gMPpektaZ50/2MRcdgIwsx631wOlPoqcIOkMYptDHel\n9ruARan9BmDj3Eo0s6Z0dKBURDwBPJGmXwfOa9Pnl8CVFdRmZg3ruyMp2/Fl783q4XMxzCzLAWFm\nWQ4IM8tyQJhZlgPCzLIGYi8GeG+FWR08gjCzLAeEmWU5IMwsywFhZlkOCBtI3mhdDQeEmWU5IMws\nywFhZlkOCDPLckCYWZYDwsyyBuJcjDLfx+ndXkcWf19JNTyCMLMsB4SZZTkgzCzLAWFmWQ4IM8ty\nQJhZlgPCzLIcEGaWVSogJO2W9IKkHZJGU9tCSY9KejXdnpjaJel2SWOSdkpaXecvYGb16WQE8QcR\nsSoihtP8RmBbRKwEtvHht3hfCqxMPyPAHVUVa2bdNZdVjHXA5jS9Gbi8pf2eKDwJLJC0ZA7LMbOG\nlA2IAH4sabukkdR2SkTsBUi3J6f2pcCelseOp7aPkDQiaVTS6Fv7D82uejOrVdmTtS6MiAlJJwOP\nSvrpNH3Vpi0Oa4jYBGwCGD5n/mH3m1nzSo0gImIi3e4DHgTOA96cXHVIt/tS93FgecvDlwETVRVs\nZt0zY0BIOlbS8ZPTwGeAF4EtwPrUbT3wUJreAlyT9masAQ5MroqYWX8ps4pxCvCgpMn+34uIH0l6\nBrhf0gbgDeDK1P8R4DJgDHgfuLbyqs2sK2YMiIh4HTinTft+YG2b9gCuq6Q6M2uUj6Q0sywHhJll\nOSDMLMsBYWZZDggzy3JAmFmWA8LMshwQZpblgDCzLAeEmWU5IMwsq+++vLeTL2Wd2rfsY/2lv9ZP\npr6vq3zPegRhZlkOCDPLckCYWZYDwsyyHBBmlqXiAlDNGj5nfjy9dfnMHafRuufBzA43tGRse8sX\nX5XiEYSZZTkgzCzLAWFmWQ4IM8tyQJhZ1sAEhPdgmFVvYALCzKrngDCzLAeEmWWVuh6EpAXAncDv\nAAH8CfAKcB+wAtgN/GFEvKviW36/QfEFvu8DfxwRz1Ze+RQzHUnZyZGWnZxPP9dtH712vYk6/kZN\n6fXfpV199dYy1vEjyo4gvgH8KCJ+m+KLfHcBG4FtEbES2JbmAS4FVqafEeCOjqsys54wY0BI+gRw\nEXAXQER8EBHvAeuAzanbZuDyNL0OuCcKTwILJC2pvHIzq12ZEcQZwFvAP0p6TtKdko4FTomIvQDp\n9uTUfymwp+Xx46ntIySNSBqVNPrW/kNz+iXMrB5lAmIesBq4IyLOBf6HD1cn2lGbtsNOGY2ITREx\nHBHDixcNlSrWzLqrTECMA+MR8VSaf4AiMN6cXHVIt/ta+reeu70MmKimXDPrplLXg5D0b8CfRsQr\nkv4GODbdtT8ibpa0EVgYETdK+hzwZYq9GOcDt0fEeTM8/y8o9or0ipOAt5suooXryeulWqC36/nN\niFjcyYPLBsQqit2cRwOvA9dSjD7uB04D3gCujIh30m7ObwKXUOzmvDYiRmd4/tFOL2RRJ9czvV6q\np5dqgcGrp9RxEBGxA2i3kLVt+gZw3WwLMrPe4SMpzSyrVwJiU9MFTOF6ptdL9fRSLTBg9fTERWvN\nrDf1ygjCzHpQ4wEh6RJJr0gaS7tLu7HMuyXtk/RiS9tCSY9KejXdnpjaJen2VN9OSasrrmW5pMcl\n7ZL0kqTrG65nvqSnJT2f6vnb1H66pKdSPfdJOjq1H5Pmx9L9K6qsJy1jKB3F+3AP1LJb0guSdkga\nTW2NvFZpGQskPSDpp+k9dEGl9UREYz/AEPAaxeHcRwPPA2d3YbkXURzs9WJL298DG9P0RuCWNH0Z\n8K8UR4iuAZ6quJYlwOo0fTzwM+DsBusRcFyaPgp4Ki3nfuCq1P5t4M/S9J8D307TVwH31fB63QB8\nD3g4zTdZy27gpCltjbxWaRmbKY5RIv0PLaiynlr/EUv8chcAW1vmbwJu6tKyV0wJiFeAJWl6CfBK\nmv4H4Op2/Wqq6yHg071QD/AbwLMUB7y9Dcyb+roBW4EL0vS81E8V1rCM4mzhi4GH05u7kVrS87YL\niEZeK+ATwH9M/R2rrKfpVYxSJ3Z1yZxOPqtCGhKfS/Gp3Vg9aUi/g+Lw+UcpRnnvRcTBNsv8dT3p\n/gPAogrLuQ24Efi/NL+owVqgOK/ox5K2SxpJbU29VrWcSNmq6YAodWJXw7pSo6TjgB8AX4mInzdZ\nT0QciohVFJ/e5wFnTbPM2uqR9HlgX0Rsb21uopYWF0bEaorrnlwn6aJp+tZdTy0nUrZqOiB66cSu\nxk4+k3QURTh8NyJ+2HQ9k6K47scTFOurCyRNHnnbusxf15PuPwF4p6ISLgS+IGk3cC/FasZtDdUC\nQERMpNt9wIMUAdrUa1X7iZRNB8QzwMq0Vfpoig1LWxqqZQuwPk2vp9gWMNl+TdoCvAY4MDl8q4Ik\nUVyMZ1dEfL0H6lms4hKDSPo48CmKK4g9DlyRqWeyziuAxyKt4M5VRNwUEcsiYgXFe+OxiPhiE7UA\nSDpW0vGT08BngBdp6LWKiP8C9kj6rdS0Fni50nqq3IAzyw0tl1FsuX8N+KsuLfP7wF7gfylSdQPF\nuuo24NV0uzD1FfCtVN8LwHDFtfw+xTBvJ7Aj/VzWYD2/CzyX6nkR+OvUfgbwNMWFDf8ZOCa1z0/z\nY+n+M2p6zT7Jh3sxGqklLff59PPS5Pu1qdcqLWMVMJper38BTqyyHh9JaWZZTa9imFkPc0CYWZYD\nwsyyHBBmluWAMLMsB4SZZTkgzCzLAWFmWf8PMUSbraGobjwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b90d5219e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(a[0].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
